{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PROYECTO II  PROGRAMA DE FORMACIÓN  MLDS AVANZADO\n",
        "## Daniel F. Benavides R. \n",
        "## Módulo V - Deep Learning\n",
        "\n",
        "### OBJETIVO\n",
        "\n",
        "El objeto del presente proyecto es poner en producción las técnicas de Deep Learning exploradas en el presente módulo.  La empresa para la cual trabajo  se encarga de la supervisión del servicio de transporte en la ciudad;  la operación es realizada por terceros y sería un gran avance  poder automatizar algunas labores que viene siendo realizada manualmente porque para las mismas no se tiene suficiente personal. Es por esto  que se plantea la utilización de un modelo de _**clasificación de audio**_ de la librería **_transformers_** a partir del cual aplicando **_Fine Tuning_** y que de ello pueda derivar algún producto con el cual se automaticen tareas de supervisión de la operación. \n",
        "\n",
        "\n",
        "Con esta idea inicia el presente, sin embargo luego de la verificación de trabajos como el realizado por  [Julien Simon](https://huggingface.co/juliensimon) quien en un ejercicio de Fine Tuning a un modelo de Facebook [facebook/wav2vec2-conformer-rel-pos-large](https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large) que realiza tarea de Automatic Speech Recognition logra llevarlo a un modelo de  Audio Classification  aplicando la técnica de  fine_tuning con el dataset _speech commands_ [juliensimon/wav2vec2-conformer-rel-pos-large-finetuned-speech-commands](https://huggingface.co/juliensimon/wav2vec2-conformer-rel-pos-large-finetuned-speech-commands). En este trabajo se encuentra que esta una meta muy alta para ser la primera vez que se hace una aproximación a un ejercicio real de *Fine Tuning*, pues requiere insumos muy complejos como un dataset muy depurado para reentrenar el modelo  además demanda gastos computacionales como el mismo autor relacionado lo explica  en [Audio Classification with Hugging Face Transformers](https://www.youtube.com/watch?v=iuvDLKql3yk)\n",
        "\n",
        "\n",
        "Habiendo explorado las bondades  de la técnica de fine tuning y  conocido más de fondo la poderosa herramienta que es  reutilizar modelos pre-entrenados para otras labores por medio del reentranamiento del cabezal;  decido tomar un modelo de clasificación de sentimientos y utilizarlo para hacer la clasificación de **SMS** no deseado por medio de la aplicación de Fine Tuning. Este ejercicio fue realizado con base en el siguiente vídeo [Fine Tuning Pretrained Model On Custom Dataset Using 🤗 Transformer\n",
        "](https://www.youtube.com/watch?v=V1-Hm2rNkik).\n",
        "\n",
        "Debido a que el objetivo del  proyecto es la aplicación  los contenidos vistos en el módulo; se tomó este ejercicio de baja complejidad como base para entender más de fondo la técnica de Fine Tuning con miras a que en próximas oportunidades y supliendo los recursos necesarios, esta técnica  se pueda llevar al plano productivo  (inicialmente planteado). Del módulo V queda la fascinación por estas dos técnicas, lo que fija como objetivo personal seguir explorando su utilización y las posibes formas de aplicación en mi labor profesional. "
      ],
      "metadata": {
        "id": "GZiMfnKVCniS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos \n",
        "\n",
        "A continuación importamos pandas, por medio del cual hacemos el respectivo cargue del dataset, delimitamos por el espacio la  etiqueta del mensaje.\n",
        "\n",
        "Luego por medio de la función _list_ convertimos el mensaje y las etiquetas en un par de listas. luego convertimos las etiquetas en una variable dummie, debido a que tenemos una salida binaria  _(el mensaje es spam o no lo es)_"
      ],
      "metadata": {
        "id": "GRFpQTqzDCLT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awPXefiYqQsF"
      },
      "source": [
        "import pandas as pd\n",
        "df=messages = pd.read_csv('SMSSpamCollection', sep='\\t',\n",
        "                           names=[\"label\", \"message\"])\n",
        "X=list(df['message'])\n",
        "y=list(df['label'])\n",
        "y=list(pd.get_dummies(y,drop_first=True)['spam'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento \n",
        "\n",
        "Ahora importamos la función *train_test_split*  del módulo *model_selection* de la librería *scikit-learn*  y por medio de este dividimos  en set de entrenamiento y prueba. Definimos el tamaño de set de prueba en 20% de la muestra. También definimos el parámetro *random_state* para efectos de controlar la generación  de los dos conjuntos de tal manera que  no sean aleatorios. \n",
        "\n",
        "Luego instalamos la librería transformers, aunque en mi caso ya lo había realizado. \n"
      ],
      "metadata": {
        "id": "1T8CpN3YDar6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLFDWda0rIKw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqOBGiGErZgj",
        "outputId": "29a8b817-fc6c-460d-edb3-315bcc8a108c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora debemos invocar los modelos de que vamos a utilizar de la librería transformers  en los siguientes pasos: \n",
        "\n",
        "* Llamamos el modelo preentrenado\n",
        "* Llamamos el tokenizador \n",
        "\n",
        "Necesitamos aplicar el tokenizador sobre nuestro conjunto de datos. \n",
        "\n",
        "Así que acontinuación llamamos de la librería transformers el tokenizador _\"DistilBertTokenizerFast\"_ luego lo definimos como nuestro **tokenizer** indicando que el mismo proviene del modelo preentrenado [_'distilbert-base-uncased'_](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.)"
      ],
      "metadata": {
        "id": "31aeQ6u-Dq-K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcNEJ6perOSs"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego  aplicamos el tokenizador que acabamos de definir sobre nuestro conjunto  de mensajes de entrenamiento y prueba. Como los SMS no tienen la misma longitud (cantidad de tokens) debemos definir los parámetros truncation y padding como True para que se obtener oraciones del mismo tamaño; uno se encarga de rellenar de ceros (padding) y el otro de truncar las oraciones más largas. Esto para obtener un conjunto y luego tensores rectangulares. "
      ],
      "metadata": {
        "id": "3RdR0eZaDyyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OL3fgLvrXvH"
      },
      "source": [
        "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se procede a importar Tensorflow para efecto de convertir en tensores los encodings generados en el paso anterior. Acá se junta cada uno a su correspondiente etiqueta. "
      ],
      "metadata": {
        "id": "7JTdRQNVD4AK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B42CTCnrrEx"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        "))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación se importan los módulos de TFDistilBertForSequenceClassification que generalmente es usado para la tarea de análisis de sentimientos. También se importan los módulos *TFTrainer* y *TFTrainingArguments* que son los encargados de entrenar el modelo para hacer las nuevas predicciones. \n"
      ],
      "metadata": {
        "id": "G3Wj2cqXD5hx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH1dupK0rzfn"
      },
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos determinado el conjunto de argumentos que serán utilizados en la etapa de entrenamiento alojados en el  objeto *training_args*   y  ahora definiremos el modelo refiriendo el modelo preentrenado que vamos a utilizar, que en este caso es  _\"distilbert-base-uncased\"_, los argumentos los argumentos antes definidos   y los dos dataset (tensores de entrenamiento y prueba); para luego  entrenar el modelo que hemos definido."
      ],
      "metadata": {
        "id": "cqmXOhkSEAgC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZvTrEcfr7k-",
        "outputId": "2eea78ff-b495-4ca8-fbf1-c8ff9fdde25a"
      },
      "source": [
        "with training_args.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset             # evaluation dataset\n",
        ")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer_tf.py:120: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora solo queda por aplicar  el entrenador que generamos al dataset de prueba, hacer la predicción, y la evaluación de las predicciones. Este procedimiento se encuentra definido en el [manual de fine-tuning](https://huggingface.co/transformers/v3.5.1/training.html) que tiene Hugging Face disponible. "
      ],
      "metadata": {
        "id": "Zerz-bv8EENp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R534aDi3xD0s",
        "outputId": "70d7abc9-4f80-4cbd-92a5-9021c133b75d"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.677699715750558}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBmI1WcxKjG",
        "outputId": "d71ec37f-0a64-40c0-bca3-4016dd69349c"
      },
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-0.02710662, -0.0705149 ],\n",
              "       [-0.06137016, -0.09839323],\n",
              "       [ 0.00922669, -0.05698987],\n",
              "       ...,\n",
              "       [-0.01441754, -0.04300478],\n",
              "       [-0.01974067, -0.06148365],\n",
              "       [-0.0413641 , -0.09714585]], dtype=float32), label_ids=array([0, 1, 0, ..., 0, 1, 0], dtype=int32), metrics={'eval_loss': 0.6777530670166015})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qc5FtM8xn9A",
        "outputId": "018fedba-979c-4aed-c6cc-7dbc06ea7beb"
      },
      "source": [
        "trainer.predict(test_dataset)[1].shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUVX_IhWxkxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a33921f-f97f-40c7-af45-ffd6ef81e7ae"
      },
      "source": [
        "output=trainer.predict(test_dataset)[1]\n",
        "output"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfCE06jQu5cI",
        "outputId": "f9ed27f6-e7d4-42f8-ac00-9ee27b3645ff"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "confusion_matrix=confusion_matrix(y_test,output)\n",
        "confusion_matrix\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[955,   0],\n",
              "       [  0, 160]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc=accuracy_score(y_test,output)\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv83DD8sl8JO",
        "outputId": "f6e31cd3-e867-426d-e751-d72641068977"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okD5we1NwhQW"
      },
      "source": [
        "trainer.save_model('tf_model')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "Se realiza el ejercicio de *fine-tuning* siguiendo cada una de los pasos que indica  HuggingFace en el  [manual de fine-tuning](https://huggingface.co/transformers/v3.5.1/training.html) y se  tiene una respuesta altamente satisfactoria del modelo con un accuracy perfecto, lo cual es viable en el entendido de que partimos de un  [modelo de fill-mask](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.) entrenado con |un corpus enorme  y lo adaptamos a un modelo de clasificación binaria. \n",
        "\n",
        "este ejercicio a pesar de ser muy poco complejo como labor final si es muy satisfactorio a nivel personal por haber podido colocar en producción  la adaptación de un modelo preentrenado en una labor absolutamente diferente de la inicial. \n",
        "\n"
      ],
      "metadata": {
        "id": "l5uH6GaqEcGA"
      }
    }
  ]
}