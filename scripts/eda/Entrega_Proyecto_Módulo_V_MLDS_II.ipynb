{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# PROYECTO II  PROGRAMA DE FORMACIN  MLDS AVANZADO\n",
        "## Daniel F. Benavides R. \n",
        "## M贸dulo V - Deep Learning\n",
        "\n",
        "### OBJETIVO\n",
        "\n",
        "El objeto del presente proyecto es poner en producci贸n las t茅cnicas de Deep Learning exploradas en el presente m贸dulo.  La empresa para la cual trabajo  se encarga de la supervisi贸n del servicio de transporte en la ciudad;  la operaci贸n es realizada por terceros y ser铆a un gran avance  poder automatizar algunas labores que viene siendo realizada manualmente porque para las mismas no se tiene suficiente personal. Es por esto  que se plantea la utilizaci贸n de un modelo de _**clasificaci贸n de audio**_ de la librer铆a **_transformers_** a partir del cual aplicando **_Fine Tuning_** y que de ello pueda derivar alg煤n producto con el cual se automaticen tareas de supervisi贸n de la operaci贸n. \n",
        "\n",
        "\n",
        "Con esta idea inicia el presente, sin embargo luego de la verificaci贸n de trabajos como el realizado por  [Julien Simon](https://huggingface.co/juliensimon) quien en un ejercicio de Fine Tuning a un modelo de Facebook [facebook/wav2vec2-conformer-rel-pos-large](https://huggingface.co/facebook/wav2vec2-conformer-rel-pos-large) que realiza tarea de Automatic Speech Recognition logra llevarlo a un modelo de  Audio Classification  aplicando la t茅cnica de  fine_tuning con el dataset _speech commands_ [juliensimon/wav2vec2-conformer-rel-pos-large-finetuned-speech-commands](https://huggingface.co/juliensimon/wav2vec2-conformer-rel-pos-large-finetuned-speech-commands). En este trabajo se encuentra que esta una meta muy alta para ser la primera vez que se hace una aproximaci贸n a un ejercicio real de *Fine Tuning*, pues requiere insumos muy complejos como un dataset muy depurado para reentrenar el modelo  adem谩s demanda gastos computacionales como el mismo autor relacionado lo explica  en [Audio Classification with Hugging Face Transformers](https://www.youtube.com/watch?v=iuvDLKql3yk)\n",
        "\n",
        "\n",
        "Habiendo explorado las bondades  de la t茅cnica de fine tuning y  conocido m谩s de fondo la poderosa herramienta que es  reutilizar modelos pre-entrenados para otras labores por medio del reentranamiento del cabezal;  decido tomar un modelo de clasificaci贸n de sentimientos y utilizarlo para hacer la clasificaci贸n de **SMS** no deseado por medio de la aplicaci贸n de Fine Tuning. Este ejercicio fue realizado con base en el siguiente v铆deo [Fine Tuning Pretrained Model On Custom Dataset Using  Transformer\n",
        "](https://www.youtube.com/watch?v=V1-Hm2rNkik).\n",
        "\n",
        "Debido a que el objetivo del  proyecto es la aplicaci贸n  los contenidos vistos en el m贸dulo; se tom贸 este ejercicio de baja complejidad como base para entender m谩s de fondo la t茅cnica de Fine Tuning con miras a que en pr贸ximas oportunidades y supliendo los recursos necesarios, esta t茅cnica  se pueda llevar al plano productivo  (inicialmente planteado). Del m贸dulo V queda la fascinaci贸n por estas dos t茅cnicas, lo que fija como objetivo personal seguir explorando su utilizaci贸n y las posibes formas de aplicaci贸n en mi labor profesional. "
      ],
      "metadata": {
        "id": "GZiMfnKVCniS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Carga de datos \n",
        "\n",
        "A continuaci贸n importamos pandas, por medio del cual hacemos el respectivo cargue del dataset, delimitamos por el espacio la  etiqueta del mensaje.\n",
        "\n",
        "Luego por medio de la funci贸n _list_ convertimos el mensaje y las etiquetas en un par de listas. luego convertimos las etiquetas en una variable dummie, debido a que tenemos una salida binaria  _(el mensaje es spam o no lo es)_"
      ],
      "metadata": {
        "id": "GRFpQTqzDCLT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awPXefiYqQsF"
      },
      "source": [
        "import pandas as pd\n",
        "df=messages = pd.read_csv('SMSSpamCollection', sep='\\t',\n",
        "                           names=[\"label\", \"message\"])\n",
        "X=list(df['message'])\n",
        "y=list(df['label'])\n",
        "y=list(pd.get_dummies(y,drop_first=True)['spam'])"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocesamiento \n",
        "\n",
        "Ahora importamos la funci贸n *train_test_split*  del m贸dulo *model_selection* de la librer铆a *scikit-learn*  y por medio de este dividimos  en set de entrenamiento y prueba. Definimos el tama帽o de set de prueba en 20% de la muestra. Tambi茅n definimos el par谩metro *random_state* para efectos de controlar la generaci贸n  de los dos conjuntos de tal manera que  no sean aleatorios. \n",
        "\n",
        "Luego instalamos la librer铆a transformers, aunque en mi caso ya lo hab铆a realizado. \n"
      ],
      "metadata": {
        "id": "1T8CpN3YDar6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLFDWda0rIKw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqOBGiGErZgj",
        "outputId": "29a8b817-fc6c-460d-edb3-315bcc8a108c"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora debemos invocar los modelos de que vamos a utilizar de la librer铆a transformers  en los siguientes pasos: \n",
        "\n",
        "* Llamamos el modelo preentrenado\n",
        "* Llamamos el tokenizador \n",
        "\n",
        "Necesitamos aplicar el tokenizador sobre nuestro conjunto de datos. \n",
        "\n",
        "As铆 que acontinuaci贸n llamamos de la librer铆a transformers el tokenizador _\"DistilBertTokenizerFast\"_ luego lo definimos como nuestro **tokenizer** indicando que el mismo proviene del modelo preentrenado [_'distilbert-base-uncased'_](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.)"
      ],
      "metadata": {
        "id": "31aeQ6u-Dq-K"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcNEJ6perOSs"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego  aplicamos el tokenizador que acabamos de definir sobre nuestro conjunto  de mensajes de entrenamiento y prueba. Como los SMS no tienen la misma longitud (cantidad de tokens) debemos definir los par谩metros truncation y padding como True para que se obtener oraciones del mismo tama帽o; uno se encarga de rellenar de ceros (padding) y el otro de truncar las oraciones m谩s largas. Esto para obtener un conjunto y luego tensores rectangulares. "
      ],
      "metadata": {
        "id": "3RdR0eZaDyyi"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OL3fgLvrXvH"
      },
      "source": [
        "train_encodings = tokenizer(X_train, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(X_test, truncation=True, padding=True)\n"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se procede a importar Tensorflow para efecto de convertir en tensores los encodings generados en el paso anterior. Ac谩 se junta cada uno a su correspondiente etiqueta. "
      ],
      "metadata": {
        "id": "7JTdRQNVD4AK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9B42CTCnrrEx"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    y_train\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    y_test\n",
        "))"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuaci贸n se importan los m贸dulos de TFDistilBertForSequenceClassification que generalmente es usado para la tarea de an谩lisis de sentimientos. Tambi茅n se importan los m贸dulos *TFTrainer* y *TFTrainingArguments* que son los encargados de entrenar el modelo para hacer las nuevas predicciones. \n"
      ],
      "metadata": {
        "id": "G3Wj2cqXD5hx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH1dupK0rzfn"
      },
      "source": [
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "\n",
        "training_args = TFTrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs\n",
        "    per_device_train_batch_size=8,  # batch size per device during training\n",
        "    per_device_eval_batch_size=8,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    logging_steps=10,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos determinado el conjunto de argumentos que ser谩n utilizados en la etapa de entrenamiento alojados en el  objeto *training_args*   y  ahora definiremos el modelo refiriendo el modelo preentrenado que vamos a utilizar, que en este caso es  _\"distilbert-base-uncased\"_, los argumentos los argumentos antes definidos   y los dos dataset (tensores de entrenamiento y prueba); para luego  entrenar el modelo que hemos definido."
      ],
      "metadata": {
        "id": "cqmXOhkSEAgC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZvTrEcfr7k-",
        "outputId": "2eea78ff-b495-4ca8-fbf1-c8ff9fdde25a"
      },
      "source": [
        "with training_args.strategy.scope():\n",
        "    model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "trainer = TFTrainer(\n",
        "    model=model,                         # the instantiated  Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset             # evaluation dataset\n",
        ")\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_layer_norm', 'vocab_projector', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_39', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer_tf.py:120: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora solo queda por aplicar  el entrenador que generamos al dataset de prueba, hacer la predicci贸n, y la evaluaci贸n de las predicciones. Este procedimiento se encuentra definido en el [manual de fine-tuning](https://huggingface.co/transformers/v3.5.1/training.html) que tiene Hugging Face disponible. "
      ],
      "metadata": {
        "id": "Zerz-bv8EENp"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R534aDi3xD0s",
        "outputId": "70d7abc9-4f80-4cbd-92a5-9021c133b75d"
      },
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.677699715750558}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBmI1WcxKjG",
        "outputId": "d71ec37f-0a64-40c0-bca3-4016dd69349c"
      },
      "source": [
        "trainer.predict(test_dataset)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PredictionOutput(predictions=array([[-0.02710662, -0.0705149 ],\n",
              "       [-0.06137016, -0.09839323],\n",
              "       [ 0.00922669, -0.05698987],\n",
              "       ...,\n",
              "       [-0.01441754, -0.04300478],\n",
              "       [-0.01974067, -0.06148365],\n",
              "       [-0.0413641 , -0.09714585]], dtype=float32), label_ids=array([0, 1, 0, ..., 0, 1, 0], dtype=int32), metrics={'eval_loss': 0.6777530670166015})"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qc5FtM8xn9A",
        "outputId": "018fedba-979c-4aed-c6cc-7dbc06ea7beb"
      },
      "source": [
        "trainer.predict(test_dataset)[1].shape"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1115,)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUVX_IhWxkxg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a33921f-f97f-40c7-af45-ffd6ef81e7ae"
      },
      "source": [
        "output=trainer.predict(test_dataset)[1]\n",
        "output"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, ..., 0, 1, 0], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfCE06jQu5cI",
        "outputId": "f9ed27f6-e7d4-42f8-ac00-9ee27b3645ff"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "confusion_matrix=confusion_matrix(y_test,output)\n",
        "confusion_matrix\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[955,   0],\n",
              "       [  0, 160]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc=accuracy_score(y_test,output)\n",
        "acc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mv83DD8sl8JO",
        "outputId": "f6e31cd3-e867-426d-e751-d72641068977"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okD5we1NwhQW"
      },
      "source": [
        "trainer.save_model('tf_model')"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusiones\n",
        "\n",
        "Se realiza el ejercicio de *fine-tuning* siguiendo cada una de los pasos que indica  HuggingFace en el  [manual de fine-tuning](https://huggingface.co/transformers/v3.5.1/training.html) y se  tiene una respuesta altamente satisfactoria del modelo con un accuracy perfecto, lo cual es viable en el entendido de que partimos de un  [modelo de fill-mask](https://huggingface.co/distilbert-base-uncased?text=Paris+is+the+%5BMASK%5D+of+France.) entrenado con |un corpus enorme  y lo adaptamos a un modelo de clasificaci贸n binaria. \n",
        "\n",
        "este ejercicio a pesar de ser muy poco complejo como labor final si es muy satisfactorio a nivel personal por haber podido colocar en producci贸n  la adaptaci贸n de un modelo preentrenado en una labor absolutamente diferente de la inicial. \n",
        "\n"
      ],
      "metadata": {
        "id": "l5uH6GaqEcGA"
      }
    }
  ]
}